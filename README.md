# ğŸ§  PokeAI â€” Autonomous PokÃ©mon Yellow Reinforcement Learning Agent

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active%20Development-success" />
  <img src="https://img.shields.io/badge/Python-3.11-blue" />
  <img src="https://img.shields.io/badge/RL-Stable--Baselines3-orange" />
  <img src="https://img.shields.io/badge/Emulator-PyBoy-purple" />
</p>

> **Architecture:** PPO (Proximal Policy Optimization) + Neuroâ€‘Symbolic State Decoding  
> **Goal:** Train an Artificial Intelligence agent capable of completing **PokÃ©mon Yellow** from scratch, with *zero prior knowledge*

---

## ğŸ¯ Project Overview

**PokeAI** is a Deep Reinforcement Learning research project focused on solving **longâ€‘horizon RPG environments**. PokÃ©mon Yellow represents a particularly challenging benchmark due to:

* Extremely sparse rewards
* Large state space
* Long-term dependencies (decisions made minutes or hours earlier)
* Partial observability from pixels alone

To overcome these challenges, PokeAI combines **visual perception** with **explicit symbolic game state extraction**, allowing the agent to both *see* and *understand* the game world.

---

## ğŸ§© Technical Description

This project implements a **Deep Reinforcement Learning (Deep RL)** architecture designed for complex Game Boyâ€“era RPGs.

Unlike purely visionâ€‘based agents that rely only on raw pixels, PokeAI uses a **Hybrid Observation Space** composed of:

1. **ğŸ‘ï¸ Vision (CNN-based)**  
   Screen processing to understand local geometry, obstacles, and transitions.

2. **ğŸ§  Memory (RAM Inspection)**  
   Direct reading of emulator memory to extract global context such as:

   * Player coordinates
   * Current map ID
   * Progress flags (e.g., badges)

This neuroâ€‘symbolic approach dramatically improves sample efficiency and stability during training.

---

## âœ¨ Key Features

* **âš¡ Accelerated Emulation**  
  Uses **PyBoy** in headless mode during training, achieving speeds of **1000+ FPS**.

* **ğŸ‘ï¸ Hybrid Observations**  
  The agent not only sees pixels, but *knows* where it is through RAMâ€‘injected state vectors.

* **ğŸ—ºï¸ Efficient Exploration**  
  Dense reward shaping based on unique visited coordinates `(x, y)` to mitigate sparse reward issues.

* **ğŸ¥ Streamerâ€‘Ready Architecture**  
  Asymmetric design allows fullâ€‘speed training in the background while a cloned instance runs at **60 FPS** for live visualization or streaming.

* **âš™ï¸ Consumerâ€‘Hardware Optimized**  
  Thread control (`OMP_NUM_THREADS=1`) and custom `SleepCallback` allow training + streaming on midâ€‘range CPUs (e.g., i5 / Ryzen 5) without system freezes.

---

## ğŸ› ï¸ Technology Stack

| Component        | Technology        | Purpose                                      |
| ---------------- | ----------------- | -------------------------------------------- |
| **Language**     | Python 3.11       | Core logic                                   |
| **RL Framework** | Stableâ€‘Baselines3 | PPO implementation & vectorized environments |
| **Emulator**     | PyBoy             | Lowâ€‘level Game Boy emulation                 |
| **Vision**       | OpenCV, NumPy     | Frame preprocessing & rendering              |
| **Logging**      | TensorBoard       | Realâ€‘time metrics (reward, loss, entropy)    |

---

## ğŸš€ Installation & Setup

### Prerequisites

* **Python 3.11** (Conda recommended)
* **PokÃ©mon Yellow ROM**  
  Must be named exactly `PokemonYellow.gb` and placed inside the `roms/` directory.

### Stepâ€‘byâ€‘Step Guide

#### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/PokeAI.git
cd PokeAI
```

#### 2ï¸âƒ£ Create a virtual environment

```bash
conda create -n pokeai python=3.11
conda activate pokeai
```

#### 3ï¸âƒ£ Install dependencies

```bash
pip install gymnasium pyboy shimmy stable-baselines3[extra] opencv-python torch-directml
```

#### 4ï¸âƒ£ Generate the initial save state (Skip Intro)

To prevent the agent from wasting hours navigating menus, create a save state immediately after the intro sequence.

```bash
python src/utils/create_initial_state.py
```

> **Instruction:** Manually play until you gain control of the character in Ashâ€™s room, then close the window.

---

## ğŸƒ Workflow & Execution

PokeAI is designed to run in **two terminals simultaneously**:

* **ğŸ§  Brain:** Highâ€‘speed training
* **ğŸ‘€ Eyes:** Realâ€‘time visualization

---

### ğŸ§  1. Training (The Brain)

Runs the PPO training loop in headless mode for maximum performance.

* **CPU Usage:** Optimized for 1â€“2 cores
* **Checkpoints:** Automatically saved to `experiments/`

```bash
python train.py
```

> Press **Ctrl + C** at any time to trigger a safe emergency checkpoint.

---

### ğŸ‘€ 2. Visualization (The Eyes)

Displays the agent playing at **60 FPS**.

* Automatically detects improved models
* Hotâ€‘reloads new checkpoints without restarting

```bash
python watch_continuous.py
```

---

### ğŸ“Š 3. Monitoring (Analytics)

Visualize reward curves, loss, and entropy in real time:

```bash
tensorboard --logdir experiments/poke_ppo_v1/logs
```

---

## ğŸ§  Agent Architecture

### Action Space

**Discrete (6 actions):**

```
[DOWN, LEFT, RIGHT, UP, A, B]
```

> `START` and `SELECT` are intentionally disabled to reduce stochastic noise and avoid menuâ€‘locking behaviors.

---

### Reward Shaping

The current reward function emphasizes **pure exploration**:

[
R_t = R_{exploration} + R_{events}
]

* **Exploration Reward:** +1.0 for each unique `(x, y)` coordinate visited per map
* **Inactivity Penalty (Implicit):** No reward for standing still forces movement through optimization pressure

---

## ğŸ“‚ Project Structure

```
PokeAI/
â”œâ”€â”€ config/                  # Hyperparameters & configs
â”œâ”€â”€ experiments/             # PPO checkpoints & TensorBoard logs
â”œâ”€â”€ roms/                    # Game ROMs (.gb)
â”œâ”€â”€ states/                  # PyBoy save states (.state)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ pokemon_env.py   # Gym wrapper (RAM, vision, smooth ticking)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ memory_reader.py # Hex-level RAM extraction
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.py                 # Training backend
â”œâ”€â”€ watch_continuous.py      # Streaming / visualization frontend
â””â”€â”€ README.md
```

---

## ğŸ”® Roadmap

* [ ] Integrate **HippoTorch / S4** for longâ€‘term memory
* [ ] Add **Visionâ€‘Language Model (VLM)** for onâ€‘screen dialogue understanding
* [ ] Badgeâ€‘aware curriculum learning
* [ ] Multiâ€‘objective reward decomposition

---

## ğŸ“œ Disclaimer

This project is for **research and educational purposes only**. You must legally own a copy of PokÃ©mon Yellow to use the ROM.

---

â­ *If you find this project interesting, consider giving it a star!*
