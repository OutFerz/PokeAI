# ğŸ§ ğŸ® indigoRL â€” Autonomous PokÃ©mon Yellow Reinforcement Learning Agent

<!-- ===================================================== -->
<!-- BANNER IMAGE -->
<!-- Recommended size: 1200x400 -->
<!-- Place at: assets/banner.png -->
<!-- ===================================================== -->

<p align="center">
  <img src="assets/banner.png" width="100%" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active%20Development-success" />
  <img src="https://img.shields.io/badge/Python-3.11-blue" />
  <img src="https://img.shields.io/badge/RL-PPO-orange" />
  <img src="https://img.shields.io/badge/Emulator-PyBoy-purple" />
  <img src="https://img.shields.io/github/stars/OutFerz/PokeAI?style=flat" />
</p>

<p align="center">
  <strong>Hybrid Vision + RAM Reinforcement Learning Agent</strong><br>
  Trained using PPO to solve a sparse, long-horizon RPG environment.
</p>

<!-- ===================================================== -->
<!-- DEMO GIF -->
<!-- Recommended: 10â€“15 seconds, <5MB -->
<!-- Record: exploration, map transitions, battles -->
<!-- Place at: assets/demo.gif -->
<!-- ===================================================== -->

<p align="center">
  <img src="assets/demo.gif" width="600" />
</p>

---

## ğŸ“š Table of Contents

- [Project Overview](#project-overview)
- [Technical Description](#technical-description)
- [Key Features](#key-features)
- [Technology Stack](#technology-stack)
- [Installation & Setup](#installation-setup)
- [Workflow & Execution](#workflow-execution)
- [Agent Architecture](#agent-architecture)
- [Project Structure](#project-structure)
- [Hardware & Scalability](#hardware-scalability)
- [Roadmap](#roadmap)
- [Disclaimer](#disclaimer)

---

## ğŸ¯ Project Overview <a id="project-overview"></a>

**IndigoRL** is a Deep Reinforcement Learning research project focused on solving **long-horizon RPG environments** using **PokÃ©mon Yellow** as a benchmark.

The game presents:
- Extremely sparse rewards
- Large state space
- Long-term dependencies
- Partial observability from pixels alone

To overcome these challenges, IndigoRL combines **visual perception** with **explicit symbolic state extraction from emulator RAM**, allowing the agent to both *see* and *understand* the game world.

---

## ğŸ§© Technical Description <a id="technical-description"></a>

- **Algorithm:** Proximal Policy Optimization (PPO)
- **Emulator:** PyBoy (headless during training)
- **Observation Space:**
  - CNN-processed screen frames
  - Structured RAM-based state vectors
- **Reward Design:**
  - Dense exploration rewards
  - Event-based progress signals
  - Implicit stagnation penalties

This neuro-symbolic approach significantly improves sample efficiency and training stability.

---

## âœ¨ Key Features <a id="key-features"></a>

- âš¡ **Accelerated Emulation** â€” 1000+ FPS headless training
- ğŸ‘ï¸ **Hybrid Observations** â€” Vision + RAM decoding
- ğŸ—ºï¸ **Dense Exploration Rewards** â€” Unique `(x, y)` tracking
- ğŸ¥ **Streamer-Ready** â€” Train in background, watch at 60 FPS
- âš™ï¸ **Hardware-Aware & Scalable** â€” CPU usage configurable

---

## ğŸ› ï¸ Technology Stack <a id="technology-stack"></a>

| Component | Technology |
|---------|-----------|
| Language | Python 3.11 |
| RL | Stable-Baselines3 (PPO) |
| Emulator | PyBoy |
| Vision | OpenCV, NumPy |
| Logging | TensorBoard |

---

## ğŸš€ Installation & Setup <a id="installation-setup"></a>

### Prerequisites
- Python 3.11 (Conda recommended)
- PokÃ©mon Yellow ROM  
  Must be named `PokemonYellow.gb` and placed in `roms/`

### Setup Steps

```bash
git clone https://github.com/OutFerz/indigoRL.git
cd indigoRL
conda create -n indigoRL python=3.11
conda activate indigoRL
pip install gymnasium pyboy shimmy stable-baselines3[extra] opencv-python torch-directml
```

### Initial Save State (Skip Intro)

```bash
python src/utils/create_initial_state.py
```

> Play manually until you gain control in Ashâ€™s room, then close the window.

---

## ğŸƒ Workflow & Execution <a id="workflow-execution"></a>

### ğŸ§  Training
```bash
python train.py
```

- Headless, high-speed PPO training
- Automatic checkpoints
- Safe interrupt via **Ctrl + C**

### ğŸ‘€ Visualization
```bash
python watch_continuous.py
```

- 60 FPS real-time playback
- Hot-reloads improved models

### ğŸ“Š Monitoring
```bash
tensorboard --logdir experiments/poke_ppo_v1/logs
```

---

## ğŸ§  Agent Architecture <a id="agent-architecture"></a>

**Action Space:**  
`[DOWN, LEFT, RIGHT, UP, A, B]`  
`START` and `SELECT` disabled to reduce noise.

**Reward Function:**
```
R_t = R_exploration + R_events
```

---

## ğŸ“‚ Project Structure <a id="project-structure"></a>

```
indigoRL/
â”œâ”€â”€ config/
â”œâ”€â”€ experiments/
â”œâ”€â”€ roms/
â”œâ”€â”€ states/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ utils/
â”œâ”€â”€ train.py
â”œâ”€â”€ watch_continuous.py
â””â”€â”€ README.md
```

---

## ğŸ’» Hardware & Scalability <a id="hardware-scalability"></a>

Default settings prioritize compatibility with consumer hardware.  
Training parallelism can be scaled by editing `train.py` or setting:

```bash
export OMP_NUM_THREADS=8
```

---

## ğŸ”® Roadmap <a id="roadmap"></a>

- [ ] Integrate **HippoTorch / S4** for long-term memory
- [ ] Add **Vision-Language Model (VLM)** for on-screen dialogue understanding

---

## ğŸ“œ Disclaimer <a id="disclaimer"></a>

This project is for **research and educational purposes only**.  
You must legally own a copy of PokÃ©mon Yellow to use the ROM.

---

â­ If you find this project interesting, consider giving it a star!